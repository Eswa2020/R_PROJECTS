---
title: "campaign_ad_data_analysis"
author: "Esther Kamau"
date: "January 12, 2021"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
#knitr::opts_chunk$set(echo = TRUE)
```



```{r global-options, include=FALSE}
knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path='Figs/',
                      echo=FALSE, warning=FALSE, message=FALSE)

```


## R Markdown

#R PROGRAMMING BASICS: EXPLOLATORY DATA ANALYSIS

#1. Defining the problem
The research problem in this case is to find out individuals that are likely to click on a blog advert based on their characteristics which include; Age Daily Time spent on site Area of residence Internet Usage Gender Country of residence

#2. Metric of Success

The metric success of this project is to identify clients likely to click on the ad after performing intense data analysis(EDA).


#3. Data Relevance
The data provided by the client is from the performance of a previous blog advert on the same website. 
The columns are as follows:
* **Daily Time Spent on the site-Integer**
* **Age of the individual browsing-Integer**
* **Area of residence Internet Usage** 
* **Gender of the browsing individual**
* **Country of Residence**

#4. Understanding the Context
A Kenyan entrepreneur has created an online cryptography course and would want to advertise it on her blog. She currently targets audiences originating from various countries. In the past, she ran ads to advertise a related course on the same blog and collected data in the process. She would now like to employ your services as a Data Science Consultant to help her identify which individuals are most likely to click on her ads.To achieve this we will perform exploratory data analysis on a dataset using R programming language.

#5. Experimental Design
* .Data Loading 
* .Data cleaning for missing values and outliers
* .Exploratory Data Analysis 
* .Conclusion-Detecting the trend in behaviour.



```{r}
# To Save the commands used during the session
#savehistory(file="mylog.Rhistory")
```


#6. Reading the Data
```{r}
# Install the following packages:
#install.packages("foreign")
#library(foreign)
#install.packages("car")
#install.packages("Hmisc")
#install.packages("reshape")
```


```{r include=FALSE}
library(dplyr)
```


```{r}
advertising <- read.csv('http://bit.ly/IPAdvertisingData',header = TRUE,
                 sep = ",",fileEncoding = "UTF-8-BOM")

```


#7. Checking the Data
```{r}
# Previewing top of th dataset
#first 6 rows
# ---
head(advertising)

```

```{r}

head(advertising, n=10)# First 10 rows of dataset
```

```{r}
head(advertising, n= -10) # All rows but the last 10
```

```{r}
tail(advertising) # Last 6 rows
```

```{r}
tail(advertising, n=10) # Last 10 rows
```

```{r}
tail(advertising, n= -10) # All rows but the first 10
```

```{r}
advertising[1:3, ] # First 3 rows
```


```{r}
advertising[1:10,1:3] # First 10 rows of data of the first 3 columns
```


```{r}
summary(advertising) # Provides basic descriptive statistics and frequencies.

```



```{r}
edit(advertising) # Open data editor one can edit contents in table if you want
```

```{r}
str(advertising) # Provides the structure of the dataset in terms of datatypes
```

```{r}
names(advertising) # Lists column names of the dataset
```
#8. Tidying the Dataset

###8a. Checking missing data
```{r}
rowSums(is.na(advertising)) # Number of missing per row
```

```{r}
colSums(is.na(advertising)) # Number of missing per column/variable
```


###8b. checking duplicates
```{r}

dim(advertising)# check dimensions
```

```{r}

unique_items <- unique(advertising)#check unique values
unique_items
```


###8c. checking missing Data

```{r}
# The function complete.cases() returns a logical vector indicating which cases are complete.
# list rows of data that have missing values
advertising[!complete.cases(advertising),]
```
###8d. Removing the missing data
```{r}
# The function na.omit() returns the object with deletion of missing values.
# Creating a new dataset without missing data
advertising <- na.omit(advertising) 
```


###8e. Renaming variables

```{r}
# Using library â€“-reshape

library(reshape)
advertising <- rename(advertising, c(Daily.Time.Spent.on.Site="time"))
advertising <- rename(advertising, c(Ad.Topic.Line="topic"))
advertising <- rename(advertising, c(Daily.Internet.Usage="usage"))
advertising <- rename(advertising, c(Clicked.on.Ad ="clicked"))
advertising <- rename(advertising, c(Timestamp="timestamp"))
advertising <- rename(advertising, c(Area.Income="income"))
advertising <- rename(advertising, c(Male="gender"))
```


```{r}
#confirming the column names are changed
head(advertising)
```



```{r}

#For ease in analysis,we convert the data into a tibble
my_data<-as_tibble(advertising)
my_data
```

###8e. Feature engineering time/date 
```{r}
#install.packages("lubridate")
#install.packages("tidyr")
library(tidyr)
library(lubridate)
advertising2 <- separate(advertising, timestamp, c("year", "month", "day"))

```

###8f. Converting data types
```{r}
#confirm data types per column
str(advertising2)
```

```{r}
#f['set_of_numbers'] = pd.to_numeric(df['set_of_numbers'], errors='coerce')
print(advertising2)
```

```{r}
# Type cast the column to 
#for practice not analysis
advertising$timestamp<- as.Date(advertising$timestamp)
```


```{r}
#confirm the date conversion
str(advertising)
```


`

```{r}
head(advertising)
```


#9. Univariate analysis


###9a.Descriptive stastistics 

```{r}
names(advertising2)
```

```{r}
#installing packages for descriptive stats
if(!require(psych)){install.packages("psych")}
if(!require(FSA)){install.packages("FSA")}
if(!require(plyr)){install.packages("plyr")}
if(!require(boot)){install.packages("boot")}
if(!require(DescTools)){install.packages("DescTools")}
```


```{r}
#DESCRIPTIVE STATISTICS FOR THE DATASET
names(advertising2)
#installing packages for descriptive stats
if(!require(psych)){install.packages("psych")}
if(!require(FSA)){install.packages("FSA")}
if(!require(plyr)){install.packages("plyr")}
if(!require(boot)){install.packages("boot")}
if(!require(DescTools)){install.packages("DescTools")}
# summary of descriptive statistics
library(psych)

describe(advertising2)
```


```{r}
#statistical measures of dispersion by grouped by gender followed by city
# Summarize function in the FSA package reports the five-number #summary,descriptive statistics for grouped ordinal data.
library(FSA)
Summarize(time~ gender + City,
          data=advertising2)
```

#
```{r}
# package give us descriptive statistics for  all variables in a data frame, listing the frequencies for levels of nominal variables; and for interval/ratio data, the minimum, 1st quartile, median, mean, 3rd quartile, and maximum
summary(advertising2)

```
###9b. Skewness and kurtosis among other statistics
```{r}
#descriptive statistics for internet usage
### Type=3 represents calculation for skewness and kurtosis
describe(advertising2$usage,
         type=3)   
```


```{r}
#descriptive statistics for time.spent
describe(advertising2$time,
         type=3)
```

```{r}
#descriptive statistics for Age
describe(advertising2$Age,
         type=3)
```

```{r}

#descriptive statistics for income
describe(advertising2$income,
         type=3)
```

```{r}
names(advertising2)
```

###9c  Univariate Visuals

```{r}
barplot(table(advertising2$time))
```

```{r}
Gender <- advertising2$gender
Gender_frequency<- table(Gender)
Gender_frequency
barplot(Gender_frequency,col="Red")
```


```{r}
Clicked <- advertising2$clicked
Clicked_frequency<- table(Clicked)
Clicked_frequency
barplot(Clicked_frequency,col="Blue")
```

```{r}
boxplot.stats(advertising2$time)
```


```{r}
counts <- table(advertising2$day)
  barplot(counts, main="Frequency of Days",  xlab="Number of Days",col="green")
```

```{r}
counts <- table(advertising2$month)
  barplot(counts, main="Frequency of months",  xlab="Number ofmonths",col="purple")

```


```{r}
barplot(table(advertising2$Age), horiz=TRUE, main="age_frequencey",col="pink")
boxplot(rt(100, 5), main="Boxplot")
stripchart(sample(1:20, 10, replace=TRUE), method="stack", main="Stripchart")
pie(table(sample(1:6, 10, replace=TRUE)), main="Piechart")
```

```{r}
screen(1)
barplot(advertising2$Age, main="Barplot")
screen(2)
boxplot(sample(1:20, 100, replace=TRUE) ~ gl(4, 25, labels=LETTERS[1:4]),
        col=rainbow(4), notch=TRUE, main="Boxplot")
screen(3)
plot(sample(1:20, 40, replace=TRUE), pch=20, xlab=NA, ylab=NA,
     main="Scatter plot")
close.screen(all.screens=TRUE)
```

#10. Bivariate Analysis



```{r}

plot(x = advertising2$day, y = advertising2$Age)
     
```


```{r}
library("ggplot2")
 #
geom_line()
ggplot(data =advertising2,aes(x=time,y=income))+
  geom_line()

```

```{r}

```

```{r}
install.packages("corrplot")
```
###10a Compute correlation matrix
```{r}
# function rcorr() [in Hmisc package] is used to compute the significance levels ##for pearson and spearman correlations. It returns both the correlation #coefficients and the p-value of the correlation for all possible pairs of columns #in the data table.

install.packages("Hmisc")
```

```{r}
#correlations
res <- cor(advertising2[0:4])
round(res, 2)
```
###10b correlations between varaiables

```{r}
#Correlation matrix with significance levels (p-value)
library("Hmisc")
#rcorr(advertising2[0:4], type = c("pearson","spearman"))
```

```{r}
res2 <- rcorr(as.matrix(advertising2[0:4]))
res2
```


```{r}
# Extract the correlation coefficients
res2$r
# Extract p-values
res2$P

```

```{r}
# flattenCorrMatrix
# cormat : matrix of the correlation coefficients
# pmat : matrix of the correlation p-values
flattenCorrMatrix <- function(cormat, pmat) {
  ut <- upper.tri(cormat)
  data.frame(
    row = rownames(cormat)[row(cormat)[ut]],
    column = rownames(cormat)[col(cormat)[ut]],
    cor  =(cormat)[ut],
    p = pmat[ut]
    )
}
```




```{r}
res2<-rcorr(as.matrix(advertising[,0:4]))
flattenCorrMatrix(res2$r, res2$P)
```


```{r}
 #visualizecorrelation matrix
#The R function symnum() replaces correlation coefficients by symbols according to the level of the correlation. It takes the correlation matrix as an argument 
symnum(res, abbr.colnames = FALSE)
```



```{r}
install.packages("corrplot")#plotting visually enhanced corr matrix
```
.
```{r}
#Positive correlations are displayed in blue and negative correlations in red #color. Color intensity and the size of the circle are proportional to the #correlation coefficients. In the right side of the correlogram, the legend color #shows the correlation coefficients and the corresponding colors
library(corrplot)
corrplot(res, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45)
```

###10c.multivariate Analysis (heatmaps)
```{r}
#The function chart.Correlation()[ in the package PerformanceAnalytics],is used to display a chart of a correlation matrix.
install.packages("PerformanceAnalytics")

```

```{r}
library("PerformanceAnalytics")
my_data <- advertising[0:4, c(0,1,2,3,4)]
chart.Correlation(my_data, histogram=TRUE, pch=19)
```
heatmap()
x : the correlation matrix to be plotted
col : color palettes
symm : logical indicating if x should be treated symmetrically; can only be true when x is a square matrix.
```{r}
# Get some colors
col<- colorRampPalette(c("blue", "white", "red"))(20)
heatmap(x = res, col = col, symm = TRUE)
```

```{r}

#gcorrplot can be installed from CRAN as follow:
install.packages("ggcorrplot")
```

```{r}
library(ggcorrplot)
```

```{r}

# Compute a correlation matrix
corr <- round(cor((advertising2[0:3])),1)
corr
```

```{r}
# Compute a matrix of correlation p-values
p.mat <- cor_pmat(advertising2[0:3])
head(p.mat[, 1:3])

```


```{r}
# Visualize the correlation matrix
# --------------------------------
# method = "square" (default)
ggcorrplot(corr)

```


```{r}
# method = "circle"
ggcorrplot(corr, method = "circle")
```

```{r}
# Reordering the correlation matrix
# --------------------------------
# using hierarchical clustering
ggcorrplot(corr, hc.order = TRUE, outline.col = "white")
```

```{r}
# Types of correlogram layout
# --------------------------------
# Get the lower triangle
ggcorrplot(corr, hc.order = TRUE, type = "lower",
     outline.col = "white")
```

```{r}
# Get the upeper triangle
ggcorrplot(corr, hc.order = TRUE, type = "upper",
     outline.col = "white")
```

```{r}
# Change colors and theme
# --------------------------------
# Argument colors
ggcorrplot(corr, hc.order = TRUE, type = "lower",
   outline.col = "white",
   ggtheme = ggplot2::theme_gray,
   colors = c("#6D9EC1", "white", "#E46726"))
```

```{r}
# Add correlation coefficients
# --------------------------------
# argument lab = TRUE
ggcorrplot(corr, hc.order = TRUE, type = "lower",
   lab = TRUE)
```

```{r}
# Add correlation significance level
# --------------------------------
# Argument p.mat
ggcorrplot(corr, hc.order = TRUE,
    type = "lower", p.mat = p.mat)
```

```{r}
# Leave blank on no significant coefficient
ggcorrplot(corr, p.mat = p.mat, hc.order = TRUE,
    type = "lower", insig = "blank")
```

#11.Observations
The data was clean and complete i.e. contained no outliers and no missing values.

The modal age was 31 years and the range was between 19 and 61.

Most of the individuals spent around 62.26 Minutes on the site.The time ranged from 32.60 minutes to 91.43.

The Average Area Income of the individuals was 55,000 which ranged between 13,996.5 and 79,484.80

The Daily Internet Usage had an average of 180 Mbs and ranged between 104.78 and 269.96

The most frequent cities were; Lake Faith and West Ryan

The most frequent Countries were; Fiji and Chad.


The number of females was more than that of male counterparts.

The number of individuals who clicked on the advert and those who didn't were equal at 500.

There are negative correlations between the following variables 
1.Area Income and Daily Time Spent on Site
2.gender and Daily Time Spent on Site
3.Clicking on the Advert and Daily Time Spent on Site. 
4.Income and Age
5.Daily Internet Usage and Age 
6.gender and Age 
7.Income and Age 
8.Income and Clicking on the Advert 
9.Daily Internet usage and Clicking on the advert.
10.gender and Clicking on the Advert

There were positive Correlations between the following variables:
1.Age and Clicking on the advert 
2.gender and Daily Internet Usage 
3.gender and  Income
4.Daily Time Spent on Site and Daily Internet Usage. 
5.Income and Daily Time Spent on Site 
6.Income and Daily Internet Usage 
7.Income and gender 
8.Age and Clicking on the Advert.


The heatmaps showed a higher correlation between




#12. Conclusuion
The dataset was appropriate. it contained no missing values and minimal outliers amongst the varaibles
Both univariate and Bivariate analysis revealed that the dataset is collinear, hence it can be analysed better by use of a classification algorithms.




